# Note:  This is an example config, see habitat_baselines/config/pointnav/ppo_pointnav.yaml
# for better hyperparameters for actual training

BASE_TASK_CONFIG_PATH: "configs/tasks/object_rearrangement_pos.yaml"
TRAINER_NAME: "rearrangement-ppo-agile"
ENV_NAME: RearrangementRLEnv
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
VIDEO_OPTION: ["disk"]
TENSORBOARD_DIR: "tb/ppo_agile_seq/seed_1/"
VIDEO_DIR: "video_dir/ppo_agile_seq/seed_1/eval"
# To evaluate on all episodes, set this to -1
TEST_EPISODE_COUNT: -1
EVAL_CKPT_PATH_DIR: "data/new_checkpoints/ppo_agile_seq/seed_1/ckpt.19.pth"
NUM_PROCESSES: 2
CHECKPOINT_FOLDER: "data/new_checkpoints/ppo_agile_seq/seed_1"
OUTPUT_LOG_DIR: data/object_rearrangement/logs
LOG_INTERVAL: 10
LOG_METRICS: True
NUM_UPDATES: 50000
CHECKPOINT_INTERVAL: 5000
DATASET_PATH: "data/datasets/object_rearrangement/v3/{split}/{split}.db"
GOAL_DATASET_PATH: "data/datasets/object_rearrangement/v3/{split}/{scene_split}.db"
RESULTS_DIR: "data/object_rearrangement/results/{split}/{type}"
EVAL_SAVE_RESULTS: True
EVAL_SAVE_RESULTS_INTERVAL: 50
EVAL:
  SPLIT: "eval"
  USE_CKPT_CONFIG: False


RL:
  REWARD_MEASURE: "rearrangement_reward"
  SUCCESS_MEASURE: "success"
  SLACK_REWARD: -0.001
  SUCCESS_REWARD: 3.5

  POLICY:
    name: "RearrangementSeqNetPolicy"

  PPO:
    # ppo params
    clip_param: 0.1
    ppo_epoch: 2
    num_mini_batch: 1
    value_loss_coef: 0.5
    entropy_coef: 0.01
    lr: 2.5e-4
    eps: 1e-5
    max_grad_norm: 0.5
    num_steps: 128
    hidden_size: 512
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: False
    reward_window_size: 50
    freeze_encoder: False
    
    # Reward model parameters
    discr_lr: 3e-4
    discr_batch_size: 2
    discr_rho: 0.75
    rescale_coef: 0.1
    discr_thr: 0.5

    # Initialize with behavior cloning baseline
    init_bc_baseline: False
    bc_baseline_ckpt: "data/new_checkpoints/resnet18_20_epochs/resnet18_20_epochs_20.ckpt"
  
MODEL:
  inflection_weight_coef: 6.6991838665799275
  ablate_depth: False
  ablate_rgb: False
  ablate_instruction: False
  INSTRUCTION_ENCODER:
    vocab_size: 128
    embedding_size: 64
    hidden_size: 128
    use_pretrained_embeddings: False
    rnn_type: "LSTM"
    final_state_only: True
    bidirectional: False
  RGB_ENCODER:
    cnn_type: "ResnetRGBEncoder"
    output_size: 256
    backbone: "resnet18"
    train_encoder: True
  DEPTH_ENCODER:
    cnn_type: "VlnResnetDepthEncoder"
    output_size: 128
    trainable: False
    backbone: "resnet50"
    ddppo_checkpoint: "data/ddppo-models/gibson-2plus-resnet50.pth"
  STATE_ENCODER:
    hidden_size: 512
    rnn_type: "GRU"
    num_recurrent_layers: 2
  SEQ2SEQ:
    use_prev_action: True
  PROGRESS_MONITOR:
    use: False
  TORCH_GPU_ID: 0

DISCRIMINATOR:
  SEQUENTIAL: True
  max_objects: 2
  ablate_depth: False
  ablate_rgb: False
  ablate_instruction: False
  INSTRUCTION_ENCODER:
    vocab_size: 128
    embedding_size: 64
    hidden_size: 128
    use_pretrained_embeddings: False
    rnn_type: "LSTM"
    final_state_only: True
    bidirectional: False
  RGB_ENCODER:
    cnn_type: "ResnetRGBEncoder"
    output_size: 256
    backbone: "resnet18"
    train_encoder: True
  DEPTH_ENCODER:
    cnn_type: "VlnResnetDepthEncoder"
    output_size: 128
    trainable: True
    backbone: "resnet50"
    ddppo_checkpoint: "NONE"
  STATE_ENCODER:
    hidden_size: 512
    rnn_type: "GRU"
    num_recurrent_layers: 2
  SEQ2SEQ:
    use_prev_action: True
  PROGRESS_MONITOR:
    use: False
  TORCH_GPU_ID: 0
